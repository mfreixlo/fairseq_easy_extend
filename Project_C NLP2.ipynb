{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPWvu4t2inJYgv/iVb25MCB"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Notebook with instructions on how to run the experiments presented in the report."],"metadata":{"id":"GCrL1C63OWnV"}},{"cell_type":"markdown","source":["This notebook was created to be ran on Google Colab.\n","\n","One can easily change the directories paths and run it locally."],"metadata":{"id":"qCdPf3Y8NoK1"}},{"cell_type":"code","source":["from google.colab import drive\n","import os, sys\n","drive.mount('/content/drive/')"],"metadata":{"id":"5jnGW0Tymt99","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1682843311205,"user_tz":-120,"elapsed":21701,"user":{"displayName":"Marta Lopes","userId":"01689050549229131237"}},"outputId":"accc0db4-6f16-4ec9-9ff5-868df4f424d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["Clone the git repository will all the extensions made to fairseq framework:"],"metadata":{"id":"qFZjDJ_YOqwz"}},{"cell_type":"code","source":["!git clone https://github.com/mfreixlo/fairseq_easy_extend.git\n","os.chdir(\"fairseq_easy_extend\")"],"metadata":{"id":"jYnKZMb4mwfT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Download the file https://drive.google.com/file/d/1SS5UT4UCKJh7QCZJkrcyrMyC55l4INsH/view and add it to the same location where the repository was cloned."],"metadata":{"id":"3b6vVlmQs41c"}},{"cell_type":"code","source":["!pip install fairseq==0.12.2\n","!pip install tensorboardX\n","!pip install sacremoses\n","!pip install wandb"],"metadata":{"id":"Sb2BwVNCm1RD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Wandb graphics\n","\n","In order to analyse the training process, we logged in wandb and tracked the reward, repetition and loss values. To do so we ran the next cell."],"metadata":{"id":"pLmgbwbGVIN3"}},{"cell_type":"code","source":["!wandb login"],"metadata":{"id":"OFrzXFkxV0ca"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Experiments"],"metadata":{"id":"tMUbhQwIPAxs"}},{"cell_type":"markdown","source":["##Tunning Hyperparameters \n","\n","To train the model for the different learning rates, we changed the line 7 in the next cell for the values in [0.0001, 0.0005, 0.001].\n","To change the batch sizes, we update the values in line 8 for one in [8, 10, 12]."],"metadata":{"id":"LruCcV2kPC17"}},{"cell_type":"code","source":["%env HYDRA_FULL_ERROR=1\n","\n","!python train.py --config-dir \"/content/fairseq_easy_extend/fairseq_easy_extend/models/nat/\" --config-name \"cmlm_config.yaml\" \\\n","checkpoint.restore_file=\"/content/drive/checkpoint_best.pt\" \\\n","checkpoint.save_dir=\"/content/fairseq_easy_extend/checkpoints/bleu_0.0001\" \\\n","task.data=\"/content/fairseq_easy_extend/iwslt14.tokenized.de-en\" \\\n","optimization.lr=[0.0001] \\\n","optimization.update_freq=[8]"],"metadata":{"id":"c2VR5Vkaea7Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Generalization\n","To analyze how a model generalizes to testing metrics different from the training one it is only needed to choose the path where the best checkpoint was saved and update the line 9 in the next cell for the metric we are testing."],"metadata":{"id":"t-x1lkKQQNhn"}},{"cell_type":"code","source":["!python decode.py \"/content/fairseq_easy_extend/iwslt14.tokenized.de-en\" --source-lang de --target-lang en \\\n","--path \"/content/fairseq_easy_extend/checkpoints/bleu_0.0001/checkpoint_best.pt\" \\\n","--task translation_lev \\\n","--iter-decode-max-iter 9 \\\n","--gen-subset test \\\n","--print-step \\\n","--remove-bpe \\\n","--tokenizer moses \\\n","--scoring bleu"],"metadata":{"id":"MTC7EA0tecup"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Reward analysis\n","For the experiment about how the reward changes the model's behaviour some changes in the code have to be done. It is enough to uncomment line 168 fairseq_easy_extend/fairseq_easy_extend/criterions/rl_criterion.py and run the cells under \"Tunning Hyperparameters\" section.\n","\n","##Repetition experiments\n","Just like for the reward analysis, to obtain the models for this experimens, it is only needed to uncomment line 182 in the same file and retrain the model.\n","\n","Each of this experiments were done independently, so we never had the lines 168 and 182 uncommented at the same time.\n","\n"],"metadata":{"id":"_vsUOAcFQxdc"}},{"cell_type":"code","source":[],"metadata":{"id":"tSuPaFmBV49D"},"execution_count":null,"outputs":[]}]}